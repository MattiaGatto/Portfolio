{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnh3c1otJQIB"
   },
   "source": [
    "---\n",
    "# Vertex AI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---\n",
    "Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://codelabs.developers.google.com/vertex-training-autopkg#5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ autopkg-codelab\n",
    "    + requirements.txt\n",
    "    + trainer/\n",
    "        + task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR='autopkg-codelab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir autopkg-codelab\n",
    "mkdir autopkg-codelab/trainer\n",
    "cd autopkg-codelab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Write trainer/task.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing autopkg-codelab/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/task.py\n",
    "import argparse\n",
    "import numpy as np \n",
    "import pandas \n",
    "import csv\n",
    "import warnings\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import pyarrow as pa\n",
    "import datasets\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast,BertTokenizer, BertModel,BertConfig, BertForSequenceClassification, BertForMultipleChoice, Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer,RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "# !pip install datasets\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,multilabel_confusion_matrix,plot_confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "iper_parameters={}\n",
    "iper_parameters['MAX_LEN'] = 512 #200\n",
    "iper_parameters['TRAIN_SIZE'] = 0.7\n",
    "iper_parameters['TRAIN_BATCH_SIZE'] = 8\n",
    "iper_parameters['VALID_BATCH_SIZE'] = 4\n",
    "iper_parameters['EPOCHS'] = 1\n",
    "iper_parameters['LEARNING_RATE'] = 1e-05\n",
    "\n",
    "CHECKPOINT = \"bert-base-multilingual-cased\"\n",
    "\n",
    "PROJECT_ID='********'\n",
    "\n",
    "def get_args():\n",
    "    '''Parses args.'''\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "      '--epochs',\n",
    "      required=False,\n",
    "      default=iper_parameters['EPOCHS'],\n",
    "      type=int,\n",
    "      help='number of epochs')\n",
    "    parser.add_argument(\n",
    "      '--job_dir',\n",
    "      required=True,\n",
    "      type=str,\n",
    "      help='bucket to store saved model, include gs://')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "# Instantiates a client\n",
    "def get_gs_content_file(f):\n",
    "    client = storage.Client()\n",
    "    bucket_name = PROJECT_ID+'-ml-vt-use-cases-topic-classification'\n",
    "    file='Dataset/'+f\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = storage.blob.Blob(file,bucket)\n",
    "    content = blob.download_as_string()\n",
    "    return BytesIO(content)\n",
    "\n",
    "def crea_label_vector(Data_Set):\n",
    "    y_label_code_reduced=[]\n",
    "    for index, row in Data_Set.iterrows():\n",
    "        row_t=(list(row['target']))\n",
    "        row_t=[int(x) for x in row_t]\n",
    "        y_label_code_reduced.append(row_t)\n",
    "    return y_label_code_reduced\n",
    "\n",
    "def stat_ds(Data_Set,y_label_code_reduced):\n",
    "    x=np.zeros(len(y_label_code_reduced[0]),dtype=int)\n",
    "    for v in y_label_code_reduced:\n",
    "        x=x+v\n",
    "    c=0\n",
    "    for v in x:\n",
    "        if v==0:\n",
    "            c+=1\n",
    "    #percentuali sull'intero dataset\n",
    "    xx=x.copy()\n",
    "    z=[]\n",
    "    for l in xx:\n",
    "        z.append(str(round((l/len(Data_Set))*100,1))+'%')\n",
    "    return x#,[float(x[:4]) for x in z]\n",
    "   \n",
    "def indx_permutati(Data_Set):\n",
    "    random.seed(42)\n",
    "    ind_ds=random.sample(range(len(Data_Set)), len(Data_Set))\n",
    "    return ind_ds\n",
    "\n",
    "def _red_list_bal(Data_Set,y_label_code_reduced):\n",
    "    c=stat_ds(Data_Set,y_label_code_reduced)\n",
    "    min_l=min(c)\n",
    "    red=[0,0,0,0]\n",
    "    ind=0\n",
    "    for i in c:\n",
    "        if i!=min_l:\n",
    "            val=c[ind]#int((i/100)*len(Data_Set))\n",
    "            red[ind]=val-min_l#int((min_l/100)*len(Data_Set))\n",
    "        else:\n",
    "            red[ind]=1\n",
    "        ind+=1\n",
    "    return red\n",
    "\n",
    "def bilancia(Data_Set,y_label_code_reduced):\n",
    "    size_prima=len(Data_Set)\n",
    "    conta1=0\n",
    "    conta2=0\n",
    "    conta3=0\n",
    "    conta4=0\n",
    "    index=0\n",
    "    red=_red_list_bal(Data_Set,y_label_code_reduced)\n",
    "    indx_perm=indx_permutati(Data_Set)\n",
    "    index_list=[]\n",
    "    row_list=Data_Set['target'].tolist()\n",
    "    for row_ind in indx_perm:\n",
    "        row=row_list[row_ind]\n",
    "        row_t=[int(x) for x in row]\n",
    "        if sum(row_t)==1:\n",
    "            if row_t[0]==1 and conta1<red[0]:\n",
    "                index_list.append(row_ind)\n",
    "                conta1+=1\n",
    "            if row_t[1]==1 and conta1<red[1]:\n",
    "                index_list.append(row_ind)\n",
    "                conta2+=1\n",
    "            if row_t[2]==1 and conta3<red[2]:\n",
    "                index_list.append(row_ind)\n",
    "                conta3+=1\n",
    "            if row_t[3]==1 and conta4<red[3]:\n",
    "                index_list.append(row_ind)\n",
    "                conta4+=1\n",
    "        index+=1\n",
    "    Data_Set.drop(index_list, inplace=True)\n",
    "    Data_Set.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_datasets_split():\n",
    "    '''Creates a tf.data.Dataset for train and evaluation.'''\n",
    "   \n",
    "    processed_text_final = pandas.read_csv(get_gs_content_file('outputfile_text_processed.csv'))\n",
    "    y_label_code = np.loadtxt(get_gs_content_file('numeric_label_topic.txt'), dtype=int)\n",
    "    y_label_code=y_label_code.tolist()\n",
    "    new_dict = np.load(get_gs_content_file(\"myDictionary_labels.npy\"), allow_pickle='TRUE')\n",
    "    new_dict.item()\n",
    "    \n",
    "    y_label_code_column=[]\n",
    "    for row in y_label_code:\n",
    "        lab=(''.join(map(str, row)))\n",
    "        y_label_code_column.append(lab)\n",
    "    Data_Set=pandas.DataFrame({'text':list(processed_text_final['text']),'target':y_label_code_column})\n",
    "    \n",
    "    delete=0\n",
    "    start_row=35\n",
    "    end_row=150 #len(Data_Set)\n",
    "    delete=len(Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))> end_row) ])+len(Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))< start_row)])\n",
    "    Data_Set = Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))<= end_row) ]\n",
    "    Data_Set = Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))>= start_row) ]\n",
    "    Data_Set = Data_Set.reset_index(drop=True)\n",
    "    \n",
    "    class_names_list=new_dict.item()\n",
    "    target_names=[x for x in class_names_list.keys()]\n",
    "    y_label_code_reduced=crea_label_vector(Data_Set)\n",
    "    \n",
    "    bilancia(Data_Set,y_label_code_reduced)\n",
    "    \n",
    "    y_label_code_reduced=crea_label_vector(Data_Set)\n",
    "    \n",
    "    X = list(Data_Set['text'])\n",
    "    y = y_label_code_reduced\n",
    "    new_df = pandas.DataFrame({'text':X, 'topic':y})\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    train_dataset=new_df.sample(frac=iper_parameters['TRAIN_SIZE'],random_state=42).reset_index(drop=True)\n",
    "    val_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "    validation_dataset=val_dataset.sample(frac=0.66,random_state=42).reset_index(drop=True)\n",
    "    test_dataset=val_dataset.drop(validation_dataset.index).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    dd = datasets.DatasetDict({'train':datasets.Dataset(pa.Table.from_pandas(train_dataset)),'validation':datasets.Dataset(pa.Table.from_pandas(validation_dataset)) , 'test':datasets.Dataset(pa.Table.from_pandas(test_dataset))})\n",
    "    \n",
    "    return dd\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    '''Tokenizes text examples.'''\n",
    "\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "def create_datasets():\n",
    "    '''Creates a tf.data.Dataset for train and evaluation.'''\n",
    "    \n",
    "    raw_datasets = create_datasets_split()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "    tokenized_datasets = raw_datasets.map((lambda examples: tokenize_function(examples, tokenizer)), batched=True)\n",
    "    \n",
    "    small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))\n",
    "    small_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(1000))\n",
    "    full_train_dataset = tokenized_datasets['train']\n",
    "    full_eval_dataset = tokenized_datasets['test']\n",
    "    \n",
    "    tf_train_dataset = small_train_dataset.remove_columns(['text']).with_format(\"tensorflow\")\n",
    "    tf_eval_dataset = small_eval_dataset.remove_columns(['text']).with_format(\"tensorflow\")\n",
    "    \n",
    "    train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "    train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"topic\"]))\n",
    "    train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
    "    \n",
    "    eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
    "    eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"topic\"]))\n",
    "    eval_tf_dataset = eval_tf_dataset.batch(8)\n",
    "    \n",
    "    return train_tf_dataset, eval_tf_dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    train_tf_dataset, eval_tf_dataset = create_datasets()\n",
    "    \n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(CHECKPOINT,num_labels=4)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=tf.metrics.BinaryAccuracy(),\n",
    "    )\n",
    "    \n",
    "    model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=args.epochs)\n",
    "    # model.fit(x=train_tf_dataset, validation_data=eval_tf_dataset, epochs=1)\n",
    "    model.save(f'{args.job_dir}/model_output')\n",
    "    model.save('model_output/')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Write a Requirements.txt file to specify additional dependencies of the ML code\n",
    "---\n",
    "These are additional dependencies for the model code not included in the predefined Vertex TensorFlow images such as TF-Hub, TensorFlow AdamW optimization and TensorFlow Text needed to import and work with pre-trained TensorFlow BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing autopkg-codelab/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/requirements.txt\n",
    "tensorflow_text\n",
    "tf-models-official\n",
    "transformers\n",
    "datasets==1.18.2\n",
    "torch==1.10.1\n",
    "torchvision==0.11.2\n",
    "torchaudio==0.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# RUN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://friendly-tower-338419-ml-vt-use-cases-topic-classification\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "PROJECT_ID='friendly-tower-338419'\n",
    "BUCKET_NAME=\"gs://${PROJECT_ID}-ml-vt-use-cases-topic-classification\"\n",
    "echo $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. RUN ON CPU\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd autopkg-codelab\n",
    "# gcloud services enable containerregistry.googleapis.com\n",
    "# PROJECT_ID='friendly-tower-338419'\n",
    "# BUCKET_NAME=\"gs://${PROJECT_ID}-models-vertexai\"\n",
    "# gsutil mb -l us-central1 $BUCKET_NAME\n",
    "\n",
    "# BASE_CPU_IMAGE=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest\n",
    "# OUTPUT_IMAGE=$PROJECT_ID-local-package-cpu:latest\n",
    "\n",
    "# gcloud ai custom-jobs local-run \\\n",
    "# --executor-image-uri=$BASE_CPU_IMAGE \\\n",
    "# --python-module=trainer.task \\\n",
    "# --output-image-uri=$OUTPUT_IMAGE \\\n",
    "# -- \\\n",
    "# --job_dir=$BUCKET_NAME \\\n",
    "# --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. RUN ON GPU\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd mattia_gatto/text-mining-use-cases/src/use_cases/use_case_topic_classification\n",
    "gcloud services enable containerregistry.googleapis.com\n",
    "PROJECT_ID='*******'\n",
    "BUCKET_NAME=\"gs://${PROJECT_ID}-vertexai-bert\"\n",
    "gsutil mb -l us-central1 $BUCKET_NAME\n",
    "\n",
    "BASE_GPU_IMAGE=us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-7:latest\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "--region=us-central1 \\\n",
    "--display-name=autopkg-codelab \\\n",
    "--args=--job_dir=$BUCKET_NAME \\\n",
    "--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=autopkg-codelab,python-module=trainer.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Automl endpoint request\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_text_classification_single_label_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"***\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    instance = predict.instance.TextClassificationPredictionInstance(\n",
    "        content=content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        # print(\" prediction:\", dict(prediction))\n",
    "        r=dict(prediction)\n",
    "    return r\n",
    "\n",
    "# [END aiplatform_predict_text_classification_single_label_sample]\n",
    "def evaluete_automl(cont=\"YOUR_TEXT_CONTENT\"):\n",
    "    dict_pred=predict_text_classification_single_label_sample(\n",
    "        project=\"****\",\n",
    "        endpoint_id=\"**********\",\n",
    "        location=\"us-central1\",\n",
    "        content=cont\n",
    "    )\n",
    "    dict_pred['confidences']\n",
    "    dict_pred['displayNames']\n",
    "    ris={dict_pred['confidences'][x]:dict_pred['displayNames'][x] for x in range (len(dict_pred['confidences']))}\n",
    "\n",
    "    x=[y for y in list(ris.keys())]\n",
    "    max_=0\n",
    "    for i in x:\n",
    "        if max_<=i:\n",
    "            max_=i\n",
    "    print('-'*50)\n",
    "    print(\"'\"+cont+\"'\",'--->',\"<< \"+ris[max_]+\" >>\")\n",
    "    print('-'*50)\n",
    "    return ris[max_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 595205226533748736\n",
      "--------------------------------------------------\n",
      "'i like chicago' ---> << GOVERNMENT/SOCIAL >>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cont=\"i like chicago\"\n",
    "label=evaluete_automl(cont=cont)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
