{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnh3c1otJQIB"
   },
   "source": [
    "---\n",
    "# Vertex AI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---\n",
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n",
      "Creating gs://friendly-tower-338419-vertex-gcs/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'friendly-tower-338419-vertex-gcs' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "# Add installed library dependencies to Python PATH variable.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID=PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# TODO: Create a globally unique Google Cloud Storage bucket for artifact storage.\n",
    "GCS_BUCKET = f\"gs://{PROJECT_ID}-vertex-gcs\"\n",
    "!gsutil mb -l $REGION $GCS_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=f\"gs://{PROJECT_ID}-ml-vt-use-cases-topic-classification\"\n",
    "BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "# TensorFlow model building libraries.\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Re-create the AdamW optimizer used in the original BERT paper.\n",
    "from official.nlp import optimization  \n",
    "\n",
    "# Libraries for data and plot model training metrics.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the Vertex AI Python SDK.\n",
    "from google.cloud import aiplatform as vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialize Vertex AI Python SDK\n",
    "---\n",
    "Initialize the Vertex AI Python SDK with your GCP Project, Region, and Google Cloud Storage Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che hai addestrato e valutato il tuo modello localmente in un Vertex Notebook come parte di un flusso di lavoro di sperimentazione, il tuo prossimo passo è addestrare e distribuire il tuo modello sulla piattaforma Vertex AI di Google Cloud.\n",
    "\n",
    "Per addestrare il tuo classificatore BERT su Google Cloud, dovrai impacchettare i tuoi script di addestramento Python e scrivere un Dockerfile che contiene le istruzioni sul codice del tuo modello ML, le dipendenze e le istruzioni di esecuzione. Potrete costruire il vostro contenitore personalizzato con Cloud Genera, le cui istruzioni sono specificati nella cloudbuild.yamle pubblicare il contenitore per il registro di Artefatto. Questo flusso di lavoro vi dà la possibilità di utilizzare lo stesso contenitore per l'esecuzione come parte di un portatile e scalabile Vertex Condotte del flusso di lavoro.\n",
    "\n",
    "Camminerai attraverso la creazione della seguente struttura di progetto per il tuo codice in modalità ML:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "|--/bert-sentiment-classifier\n",
    "   |--/trainer\n",
    "      |--__init__.py\n",
    "      |--task.py\n",
    "   |--Dockerfile\n",
    "   |--cloudbuild.yaml\n",
    "   |--requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a model.py training script\n",
    "Innanzitutto, riordinerai il codice di addestramento del modello TensorFlow locale dall'alto in uno script di addestramento.\n",
    "2. Scrivi un file task.py come punto di ingresso per il contenitore del modello personalizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘bertclassifier’: File exists\n",
      "mkdir: cannot create directory ‘bertclassifier/trainer’: File exists\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"bertclassifier\"\n",
    "!mkdir $MODEL_DIR\n",
    "!mkdir $MODEL_DIR/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Write trainer/task.py\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bertclassifier/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/task.py\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import argparse\n",
    "# from trainer import model\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from official.nlp import optimization\n",
    "import argparse\n",
    "import numpy as np \n",
    "import pandas \n",
    "import csv\n",
    "import warnings\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import pyarrow as pa\n",
    "import datasets\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast,BertTokenizer, BertModel,BertConfig, BertForSequenceClassification, BertForMultipleChoice, Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer,RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "# !pip install datasets\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,multilabel_confusion_matrix,plot_confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID=PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME=f\"gs://{PROJECT_ID}-ml-vt-use-cases-topic-classification\"\n",
    "\n",
    "# DATA_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "# LOCAL_DATA_DIR = './tmp/data'\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "HPARAMS = {\n",
    "    \"seed\": 42,\n",
    "    \"batch-size\": 32,\n",
    "    # TF Hub BERT modules.\n",
    "    \"tfhub-bert-preprocessor\": \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "    \"tfhub-bert-encoder\": \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\",\n",
    "    \"epochs\": 3,\n",
    "    \"initial-learning-rate\": 3e-5,\n",
    "    \"dropout\": 0.1 ,\n",
    "    \"model-dir\":\"./bert-sentiment-classifier-local\"\n",
    "}\n",
    "\n",
    "def get_gs_content_file(f):\n",
    "    client = storage.Client()\n",
    "    bucket_name = PROJECT_ID+'-ml-vt-use-cases-topic-classification'\n",
    "    file='Dataset/'+f\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = storage.blob.Blob(file,bucket)\n",
    "    content = blob.download_as_string()\n",
    "    return BytesIO(content)\n",
    "\n",
    "def crea_label_vector(Data_Set):\n",
    "    y_label_code_reduced=[]\n",
    "    for index, row in Data_Set.iterrows():\n",
    "        row_t=(list(row['target']))\n",
    "        row_t=[int(x) for x in row_t]\n",
    "        y_label_code_reduced.append(row_t)\n",
    "    return y_label_code_reduced\n",
    "\n",
    "def stat_ds(Data_Set,y_label_code_reduced):\n",
    "    x=np.zeros(len(y_label_code_reduced[0]),dtype=int)\n",
    "    for v in y_label_code_reduced:\n",
    "        x=x+v\n",
    "    c=0\n",
    "    for v in x:\n",
    "        if v==0:\n",
    "            c+=1\n",
    "    #percentuali sull'intero dataset\n",
    "    xx=x.copy()\n",
    "    z=[]\n",
    "    for l in xx:\n",
    "        z.append(str(round((l/len(Data_Set))*100,1))+'%')\n",
    "    return x#,[float(x[:4]) for x in z]\n",
    "   \n",
    "def indx_permutati(Data_Set):\n",
    "    random.seed(42)\n",
    "    ind_ds=random.sample(range(len(Data_Set)), len(Data_Set))\n",
    "    return ind_ds\n",
    "\n",
    "def _red_list_bal(Data_Set,y_label_code_reduced):\n",
    "    c=stat_ds(Data_Set,y_label_code_reduced)\n",
    "    min_l=min(c)\n",
    "    red=[0,0,0,0]\n",
    "    ind=0\n",
    "    for i in c:\n",
    "        if i!=min_l:\n",
    "            val=c[ind]#int((i/100)*len(Data_Set))\n",
    "            red[ind]=val-min_l#int((min_l/100)*len(Data_Set))\n",
    "        else:\n",
    "            red[ind]=1\n",
    "        ind+=1\n",
    "    return red\n",
    "\n",
    "def bilancia(Data_Set,y_label_code_reduced):\n",
    "    size_prima=len(Data_Set)\n",
    "    conta1=0\n",
    "    conta2=0\n",
    "    conta3=0\n",
    "    conta4=0\n",
    "    index=0\n",
    "    red=_red_list_bal(Data_Set,y_label_code_reduced)\n",
    "    indx_perm=indx_permutati(Data_Set)\n",
    "    index_list=[]\n",
    "    row_list=Data_Set['target'].tolist()\n",
    "    for row_ind in indx_perm:\n",
    "        row=row_list[row_ind]\n",
    "        row_t=[int(x) for x in row]\n",
    "        if sum(row_t)==1:\n",
    "            if row_t[0]==1 and conta1<red[0]:\n",
    "                index_list.append(row_ind)\n",
    "                conta1+=1\n",
    "            if row_t[1]==1 and conta1<red[1]:\n",
    "                index_list.append(row_ind)\n",
    "                conta2+=1\n",
    "            if row_t[2]==1 and conta3<red[2]:\n",
    "                index_list.append(row_ind)\n",
    "                conta3+=1\n",
    "            if row_t[3]==1 and conta4<red[3]:\n",
    "                index_list.append(row_ind)\n",
    "                conta4+=1\n",
    "        index+=1\n",
    "    Data_Set.drop(index_list, inplace=True)\n",
    "    Data_Set.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_datasets_split():\n",
    "    '''Creates a tf.data.Dataset for train and evaluation.'''\n",
    "   \n",
    "    processed_text_final = pandas.read_csv(get_gs_content_file('outputfile_text_processed.csv'))\n",
    "    y_label_code = np.loadtxt(get_gs_content_file('numeric_label_topic.txt'), dtype=int)\n",
    "    y_label_code=y_label_code.tolist()\n",
    "    new_dict = np.load(get_gs_content_file(\"myDictionary_labels.npy\"), allow_pickle='TRUE')\n",
    "    new_dict.item()\n",
    "    \n",
    "    y_label_code_column=[]\n",
    "    for row in y_label_code:\n",
    "        lab=(''.join(map(str, row)))\n",
    "        y_label_code_column.append(lab)\n",
    "    Data_Set=pandas.DataFrame({'text':list(processed_text_final['text']),'target':y_label_code_column})\n",
    "    \n",
    "    delete=0\n",
    "    start_row=35\n",
    "    end_row=150 #len(Data_Set)\n",
    "    delete=len(Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))> end_row) ])+len(Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))< start_row)])\n",
    "    Data_Set = Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))<= end_row) ]\n",
    "    Data_Set = Data_Set[(Data_Set['text'].map(lambda x: len(x.split(' ')))>= start_row) ]\n",
    "    Data_Set = Data_Set.reset_index(drop=True)\n",
    "    \n",
    "    class_names_list=new_dict.item()\n",
    "    target_names=[x for x in class_names_list.keys()]\n",
    "    y_label_code_reduced=crea_label_vector(Data_Set)\n",
    "    \n",
    "    bilancia(Data_Set,y_label_code_reduced)\n",
    "    \n",
    "    y_label_code_reduced=crea_label_vector(Data_Set)\n",
    "    \n",
    "    X = list(Data_Set['text'])\n",
    "    y = y_label_code_reduced\n",
    "    new_df = pandas.DataFrame({'text':X, 'topic':y})\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    train_dataset=new_df.sample(frac=0.7,random_state=42).reset_index(drop=True)\n",
    "    val_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "    validation_dataset=val_dataset.sample(frac=0.66,random_state=42).reset_index(drop=True)\n",
    "    test_dataset=val_dataset.drop(validation_dataset.index).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    dd = datasets.DatasetDict({'train':datasets.Dataset(pa.Table.from_pandas(train_dataset)),'validation':datasets.Dataset(pa.Table.from_pandas(validation_dataset)) , 'test':datasets.Dataset(pa.Table.from_pandas(test_dataset))})\n",
    "    \n",
    "    return dd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_text_classifier(hparams, optimizer):\n",
    "\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    print(text_input)\n",
    "    preprocessor = hub.KerasLayer(hparams['tfhub-bert-preprocessor'], name='preprocessing')\n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    encoder = hub.KerasLayer(hparams['tfhub-bert-encoder'], trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    classifier = outputs['pooled_output']\n",
    "    classifier = tf.keras.layers.Dropout(hparams['dropout'], name='dropout')(classifier)\n",
    "    classifier = tf.keras.layers.Dense(4, activation=None, name='classifier')(classifier)\n",
    "    model = tf.keras.Model(text_input, classifier, name='bert-classifier')\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_evaluate(hparams):\n",
    "\n",
    "    dataset_dir=create_datasets_split()\n",
    "    \n",
    "    arg_text = tf.convert_to_tensor(dataset_dir['train']['text'])\n",
    "    arg_topic = tf.convert_to_tensor([list(y) for y in dataset_dir['train']['topic']], dtype=tf.int32)\n",
    "    train_ds=(arg_text,arg_topic)\n",
    "    arg_text = tf.convert_to_tensor(dataset_dir['validation']['text'])\n",
    "    arg_topic = tf.convert_to_tensor([list(y) for y in dataset_dir['validation']['topic']], dtype=tf.int32)\n",
    "    val_ds=(arg_text,arg_topic)\n",
    "    arg_text = tf.convert_to_tensor(dataset_dir['test']['text'])\n",
    "    arg_topic = tf.convert_to_tensor([list(y) for y in dataset_dir['test']['topic']], dtype=tf.int32)\n",
    "    test_ds=(arg_text,arg_topic)\n",
    "    \n",
    "    \n",
    "    epochs = hparams['epochs']\n",
    "    steps_per_epoch =len(train_ds)# tf.data.experimental.cardinality(train_ds.value).numpy()\n",
    "    n_train_steps = steps_per_epoch * epochs\n",
    "    n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "    \n",
    "    optimizer = optimization.create_optimizer(init_lr=hparams['initial-learning-rate'],\n",
    "                                              num_train_steps=n_train_steps,\n",
    "                                              num_warmup_steps=n_warmup_steps,\n",
    "                                              optimizer_type='adamw')    \n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_text_classifier(hparams=hparams, optimizer=optimizer)\n",
    "        logging.info(model.summary())\n",
    "    history = model.fit(x=train_ds[0],y=train_ds[1], validation_data=val_ds, epochs=epochs)  \n",
    "    \n",
    "    logging.info(\"Test accuracy: %s\", model.evaluate(x=test_ds[0],y=test_ds[1]))\n",
    "\n",
    "    # Export Keras model in TensorFlow SavedModel format.\n",
    "    model.save(hparams['model-dir'])\n",
    "    \n",
    "    return history\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Vertex custom container training args. These are set by Vertex AI during training but can also be overwritten.\n",
    "    parser.add_argument('--model-dir', dest='model-dir',\n",
    "                        default=os.environ['AIP_MODEL_DIR'], type=str, help='GCS URI for saving model artifacts.')\n",
    "\n",
    "    # Model training args.\n",
    "    parser.add_argument('--tfhub-bert-preprocessor', dest='tfhub-bert-preprocessor', \n",
    "                        default='https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', type=str, help='TF-Hub URL.')\n",
    "    parser.add_argument('--tfhub-bert-encoder', dest='tfhub-bert-encoder', \n",
    "                        default='https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2', type=str, help='TF-Hub URL.')    \n",
    "    parser.add_argument('--initial-learning-rate', dest='initial-learning-rate', default=1e-5, type=float, help='Learning rate for optimizer.')\n",
    "    parser.add_argument('--epochs', dest='epochs', default=1, type=int, help='Training iterations.')    \n",
    "    parser.add_argument('--batch-size', dest='batch-size', default=8, type=int, help='Number of examples during each training iteration.')    \n",
    "    parser.add_argument('--dropout', dest='dropout', default=0.1, type=float, help='Float percentage of DNN nodes [0,1] to drop for regularization.')    \n",
    "    parser.add_argument('--seed', dest='seed', default=42, type=int, help='Random number generator seed to prevent overlap between train and val sets.')\n",
    "    \n",
    "    \n",
    "    # h = {'initial-learning-rate':3e-5,'epochs':1,'seed':42,'batch-size':32,'dropout':0.1,'tfhub-bert-encoder':'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2',\n",
    "    #      'tfhub-bert-preprocessor':'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3','model-dir':'model-dir'}\n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "    # hparams=h\n",
    "    train_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Write a Requirements.txt file to specify additional dependencies of the ML code\n",
    "---\n",
    "These are additional dependencies for the model code not included in the predefined Vertex TensorFlow images such as TF-Hub, TensorFlow AdamW optimization and TensorFlow Text needed to import and work with pre-trained TensorFlow BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bertclassifier/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/requirements.txt\n",
    "tensorflow_text\n",
    "tf-models-official\n",
    "transformers\n",
    "datasets==1.18.2\n",
    "torch==1.10.1\n",
    "torchvision==0.11.2\n",
    "torchaudio==0.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Write a Dockerfile \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bertclassifier/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/Dockerfile\n",
    "# Specifies base image and tag.\n",
    "# https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\n",
    "\n",
    "# Sets the container working directory.\n",
    "WORKDIR /root\n",
    "\n",
    "# Copies the requirements.txt into the container to reduce network calls.\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Installs additional packages.\n",
    "RUN pip3 install -U -r requirements.txt\n",
    "\n",
    "# b/203105209 Removes unneeded file from TF2.5 CPU image for python_module CustomJob training. \n",
    "# Will be removed on subsequent public Vertex images.\n",
    "RUN rm -rf /var/sitecustomize/sitecustomize.py\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY . /trainer\n",
    "\n",
    "# Sets the container working directory.\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## RUN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://friendly-tower-338419-ml-vt-use-cases-topic-classification\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "PROJECT_ID='friendly-tower-338419'\n",
    "BUCKET_NAME=\"gs://${PROJECT_ID}-ml-vt-use-cases-topic-classification\"\n",
    "echo $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Artifact Registry for custom container images\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create request issued for: [bertclassifier]\n",
      "Waiting for operation [projects/friendly-tower-338419/locations/us-central1/operations/2381bc20-a9a0-4d81-a1a7-ff30083d55de] to complete...\n",
      ".....done.\n",
      "Created repository [bertclassifier].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud artifacts repositories create \"bertclassifier\" --repository-format=\"docker\" --location=us-central1 --description=\"Artifact registry for ML custom training images for multilingual classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create cloudbuild.yaml instructions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_REGISTRY=MODEL_DIR\n",
    "IMAGE_NAME=MODEL_DIR\n",
    "IMAGE_TAG=\"latest\"\n",
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "cloudbuild_yaml = f\"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', '{IMAGE_URI}', '.' ]\n",
    "images: \n",
    "- '{IMAGE_URI}'\"\"\"\n",
    "\n",
    "with open(f\"{MODEL_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build and submit your container image to Artifact Registry using Cloud Build\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 12.8 KiB before compression.\n",
      "Uploading tarball of [bertclassifier] to [gs://friendly-tower-338419_cloudbuild/source/1645107995.222383-7ea5a3514cea4fa2ba02954fd0face40.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/friendly-tower-338419/locations/global/builds/497a7b03-64f2-47f3-b4d0-b7e4d8e41c2f].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/497a7b03-64f2-47f3-b4d0-b7e4d8e41c2f?project=480829964338].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"497a7b03-64f2-47f3-b4d0-b7e4d8e41c2f\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://friendly-tower-338419_cloudbuild/source/1645107995.222383-7ea5a3514cea4fa2ba02954fd0face40.tgz#1645107996087047\n",
      "Copying gs://friendly-tower-338419_cloudbuild/source/1645107995.222383-7ea5a3514cea4fa2ba02954fd0face40.tgz#1645107996087047...\n",
      "/ [1 files][  4.7 KiB/  4.7 KiB]                                                \n",
      "Operation completed over 1 objects/4.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  17.92kB\n",
      "Step 1/8 : FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\n",
      "latest: Pulling from vertex-ai/training/tf-cpu.2-6\n",
      "feac53061382: Pulling fs layer\n",
      "7270e73e2667: Pulling fs layer\n",
      "bdaae651af3f: Pulling fs layer\n",
      "1148834bd9d8: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "1148834bd9d8: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "f904f76abbe4: Pulling fs layer\n",
      "0976a87c9f48: Pulling fs layer\n",
      "076ba86c3f40: Pulling fs layer\n",
      "41c88a594c8b: Pulling fs layer\n",
      "44b22e02eb44: Pulling fs layer\n",
      "ba0435ea2ef5: Pulling fs layer\n",
      "69afaffe6a49: Pulling fs layer\n",
      "3f8a33bd180a: Pulling fs layer\n",
      "2a00d28ce456: Pulling fs layer\n",
      "9fcf01642b85: Pulling fs layer\n",
      "07c36a7fd024: Pulling fs layer\n",
      "df1aa6147fdd: Pulling fs layer\n",
      "1be9a569c2f6: Pulling fs layer\n",
      "89c9e74c9c4c: Pulling fs layer\n",
      "9c1c95c4a10c: Pulling fs layer\n",
      "43922376e3f2: Pulling fs layer\n",
      "b2867b553c8f: Pulling fs layer\n",
      "82592f4e6709: Pulling fs layer\n",
      "fe3eddb05a06: Pulling fs layer\n",
      "442cd8558cbf: Pulling fs layer\n",
      "47df1213eb7b: Pulling fs layer\n",
      "d1f8423fa1f7: Pulling fs layer\n",
      "ad9628131dde: Pulling fs layer\n",
      "d1427baf5421: Pulling fs layer\n",
      "02343499fe04: Pulling fs layer\n",
      "2cc8ae6b7bc5: Pulling fs layer\n",
      "b191036094fa: Pulling fs layer\n",
      "6bf34d10d7a4: Pulling fs layer\n",
      "f904f76abbe4: Waiting\n",
      "0976a87c9f48: Waiting\n",
      "076ba86c3f40: Waiting\n",
      "41c88a594c8b: Waiting\n",
      "44b22e02eb44: Waiting\n",
      "ba0435ea2ef5: Waiting\n",
      "69afaffe6a49: Waiting\n",
      "3f8a33bd180a: Waiting\n",
      "2a00d28ce456: Waiting\n",
      "9fcf01642b85: Waiting\n",
      "07c36a7fd024: Waiting\n",
      "df1aa6147fdd: Waiting\n",
      "1be9a569c2f6: Waiting\n",
      "89c9e74c9c4c: Waiting\n",
      "9c1c95c4a10c: Waiting\n",
      "43922376e3f2: Waiting\n",
      "b2867b553c8f: Waiting\n",
      "82592f4e6709: Waiting\n",
      "fe3eddb05a06: Waiting\n",
      "442cd8558cbf: Waiting\n",
      "47df1213eb7b: Waiting\n",
      "d1f8423fa1f7: Waiting\n",
      "ad9628131dde: Waiting\n",
      "02343499fe04: Waiting\n",
      "2cc8ae6b7bc5: Waiting\n",
      "b191036094fa: Waiting\n",
      "6bf34d10d7a4: Waiting\n",
      "d1427baf5421: Waiting\n",
      "7270e73e2667: Download complete\n",
      "feac53061382: Verifying Checksum\n",
      "feac53061382: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "1148834bd9d8: Verifying Checksum\n",
      "1148834bd9d8: Download complete\n",
      "f904f76abbe4: Verifying Checksum\n",
      "f904f76abbe4: Download complete\n",
      "076ba86c3f40: Verifying Checksum\n",
      "076ba86c3f40: Download complete\n",
      "41c88a594c8b: Verifying Checksum\n",
      "41c88a594c8b: Download complete\n",
      "44b22e02eb44: Verifying Checksum\n",
      "44b22e02eb44: Download complete\n",
      "ba0435ea2ef5: Verifying Checksum\n",
      "ba0435ea2ef5: Download complete\n",
      "69afaffe6a49: Verifying Checksum\n",
      "69afaffe6a49: Download complete\n",
      "3f8a33bd180a: Verifying Checksum\n",
      "3f8a33bd180a: Download complete\n",
      "0976a87c9f48: Verifying Checksum\n",
      "0976a87c9f48: Download complete\n",
      "2a00d28ce456: Verifying Checksum\n",
      "2a00d28ce456: Download complete\n",
      "bdaae651af3f: Verifying Checksum\n",
      "bdaae651af3f: Download complete\n",
      "9fcf01642b85: Verifying Checksum\n",
      "9fcf01642b85: Download complete\n",
      "1be9a569c2f6: Verifying Checksum\n",
      "1be9a569c2f6: Download complete\n",
      "feac53061382: Pull complete\n",
      "df1aa6147fdd: Verifying Checksum\n",
      "df1aa6147fdd: Download complete\n",
      "7270e73e2667: Pull complete\n",
      "9c1c95c4a10c: Verifying Checksum\n",
      "9c1c95c4a10c: Download complete\n",
      "89c9e74c9c4c: Verifying Checksum\n",
      "89c9e74c9c4c: Download complete\n",
      "43922376e3f2: Verifying Checksum\n",
      "43922376e3f2: Download complete\n",
      "b2867b553c8f: Verifying Checksum\n",
      "b2867b553c8f: Download complete\n",
      "82592f4e6709: Verifying Checksum\n",
      "82592f4e6709: Download complete\n",
      "442cd8558cbf: Verifying Checksum\n",
      "442cd8558cbf: Download complete\n",
      "47df1213eb7b: Verifying Checksum\n",
      "47df1213eb7b: Download complete\n",
      "d1f8423fa1f7: Download complete\n",
      "ad9628131dde: Verifying Checksum\n",
      "ad9628131dde: Download complete\n",
      "d1427baf5421: Verifying Checksum\n",
      "d1427baf5421: Download complete\n",
      "fe3eddb05a06: Verifying Checksum\n",
      "fe3eddb05a06: Download complete\n",
      "02343499fe04: Verifying Checksum\n",
      "02343499fe04: Download complete\n",
      "2cc8ae6b7bc5: Verifying Checksum\n",
      "2cc8ae6b7bc5: Download complete\n",
      "b191036094fa: Verifying Checksum\n",
      "b191036094fa: Download complete\n",
      "6bf34d10d7a4: Verifying Checksum\n",
      "6bf34d10d7a4: Download complete\n",
      "07c36a7fd024: Verifying Checksum\n",
      "07c36a7fd024: Download complete\n",
      "bdaae651af3f: Pull complete\n",
      "1148834bd9d8: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "f904f76abbe4: Pull complete\n",
      "0976a87c9f48: Pull complete\n",
      "076ba86c3f40: Pull complete\n",
      "41c88a594c8b: Pull complete\n",
      "44b22e02eb44: Pull complete\n",
      "ba0435ea2ef5: Pull complete\n",
      "69afaffe6a49: Pull complete\n",
      "3f8a33bd180a: Pull complete\n",
      "2a00d28ce456: Pull complete\n",
      "9fcf01642b85: Pull complete\n",
      "07c36a7fd024: Pull complete\n",
      "df1aa6147fdd: Pull complete\n",
      "1be9a569c2f6: Pull complete\n",
      "89c9e74c9c4c: Pull complete\n",
      "9c1c95c4a10c: Pull complete\n",
      "43922376e3f2: Pull complete\n",
      "b2867b553c8f: Pull complete\n",
      "82592f4e6709: Pull complete\n",
      "fe3eddb05a06: Pull complete\n",
      "442cd8558cbf: Pull complete\n",
      "47df1213eb7b: Pull complete\n",
      "d1f8423fa1f7: Pull complete\n",
      "ad9628131dde: Pull complete\n",
      "d1427baf5421: Pull complete\n",
      "02343499fe04: Pull complete\n",
      "2cc8ae6b7bc5: Pull complete\n",
      "b191036094fa: Pull complete\n",
      "6bf34d10d7a4: Pull complete\n",
      "Digest: sha256:9579635bb22888024df279dcda1a82f23dfdb42022f8a352eb8774e2c0b7a3c6\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\n",
      " ---> 6828de55ad33\n",
      "Step 2/8 : WORKDIR /root\n",
      " ---> Running in 290faaa2bd17\n",
      "Removing intermediate container 290faaa2bd17\n",
      " ---> 50fb37137b8f\n",
      "Step 3/8 : COPY requirements.txt .\n",
      " ---> ad764d891cc8\n",
      "Step 4/8 : RUN pip3 install -U -r requirements.txt\n",
      " ---> Running in 85c44faf4721\n",
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tf-models-official\n",
      "  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "Collecting datasets==1.18.2\n",
      "  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
      "Collecting torch==1.10.1\n",
      "  Downloading torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
      "Collecting torchvision==0.11.2\n",
      "  Downloading torchvision-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
      "Collecting torchaudio==0.10.1\n",
      "  Downloading torchaudio-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (5.0.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (2021.7.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (1.19.5)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (3.7.4.post0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (4.6.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.18.2->-r requirements.txt (line 4)) (0.3.4)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.10.1->-r requirements.txt (line 5)) (3.10.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.11.2->-r requirements.txt (line 6)) (8.3.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_text->-r requirements.txt (line 1)) (0.12.0)\n",
      "Collecting tensorflow<2.9,>=2.8.0\n",
      "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (2.15.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (4.3.0)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: pyyaml<6.0,>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (3.4.2)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.1-py2.py3-none-any.whl (234 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (1.7.1)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (5.8.0)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official->-r requirements.txt (line 2)) (4.1.3)\n",
      "Collecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 3)) (2021.8.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 3)) (3.0.12)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (0.1.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (1.31.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (1.34.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (3.16.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (1.53.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (4.7.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official->-r requirements.txt (line 2)) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official->-r requirements.txt (line 2)) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official->-r requirements.txt (line 2)) (1.26.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official->-r requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.18.2->-r requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.18.2->-r requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.12)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (0.2.0)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (3.1.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.38.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official->-r requirements.txt (line 2)) (0.1.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==1.18.2->-r requirements.txt (line 4)) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==1.18.2->-r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==1.18.2->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==1.18.2->-r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.18.2->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official->-r requirements.txt (line 2)) (1.3)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu->tf-models-official->-r requirements.txt (line 2)) (0.4.4)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (8.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official->-r requirements.txt (line 2)) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official->-r requirements.txt (line 2)) (2.2.0)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official->-r requirements.txt (line 2)) (0.18.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official->-r requirements.txt (line 2)) (5.2.2)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official->-r requirements.txt (line 2)) (2.3)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Building wheels for collected packages: kaggle, py-cpuinfo, pycocotools, seqeval\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73052 sha256=43e88388511446623d069e9a920aae8760983a9724e08b5c94ef81e45f764d6f\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22244 sha256=a92894ce6e36da661790d092b16f7f8993fb8adf11d58461d2497aafa45db580\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for pycocotools (PEP 517): started\n",
      "  Building wheel for pycocotools (PEP 517): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp37-cp37m-linux_x86_64.whl size=273655 sha256=a67c10919d032d187181eac13009b2d449f9e96308458e49e1d9875657b2dabc\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/5f/fa/f011e578cc76e1fc5be8dce30b3eb9fd00f337e744b3bba59b\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16171 sha256=8a03798fa4b5e3c3d1e92bb2b542710da13483849e3a33f30bc3e316e98023b9\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built kaggle py-cpuinfo pycocotools seqeval\n",
      "Installing collected packages: numpy, absl-py, tf-estimator-nightly, tensorflow-io-gcs-filesystem, tensorboard, libclang, keras, typeguard, tqdm, tensorflow, tabulate, portalocker, xxhash, torch, tokenizers, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacremoses, sacrebleu, pycocotools, py-cpuinfo, opencv-python-headless, multiprocess, kaggle, huggingface-hub, gin-config, Cython, transformers, torchvision, torchaudio, tf-models-official, datasets\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.6.0\n",
      "    Uninstalling keras-2.6.0:\n",
      "      Successfully uninstalled keras-2.6.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.0\n",
      "    Uninstalling tqdm-4.62.0:\n",
      "      Successfully uninstalled tqdm-4.62.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 1.2.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.15.0 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.23.3 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires numpy<1.20,>=1.16, but you have numpy 1.21.5 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.23.3 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires numpy<1.20,>=1.16, but you have numpy 1.21.5 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2, but you have tensorflow 2.8.0 which is incompatible.\n",
      "tensorflow-io 0.18.0 requires tensorflow<2.6.0,>=2.5.0, but you have tensorflow 2.8.0 which is incompatible.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, but you have tensorflow-io-gcs-filesystem 0.24.0 which is incompatible.\n",
      "apache-beam 2.31.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
      "apache-beam 2.31.0 requires numpy<1.21.0,>=1.14.3, but you have numpy 1.21.5 which is incompatible.\n",
      "apache-beam 2.31.0 requires pyarrow<5.0.0,>=0.15.1, but you have pyarrow 5.0.0 which is incompatible.\n",
      "apache-beam 2.31.0 requires typing-extensions<3.8.0,>=3.7.0, but you have typing-extensions 3.10.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mSuccessfully installed Cython-0.29.28 absl-py-0.12.0 datasets-1.18.2 gin-config-0.5.0 huggingface-hub-0.4.0 kaggle-1.5.12 keras-2.8.0 libclang-13.0.0 multiprocess-0.70.12.2 numpy-1.21.5 opencv-python-headless-4.5.5.62 portalocker-2.3.2 py-cpuinfo-8.0.0 pycocotools-2.0.4 sacrebleu-2.0.0 sacremoses-0.0.47 sentencepiece-0.1.96 seqeval-1.2.2 tabulate-0.8.9 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-addons-0.16.1 tensorflow-io-gcs-filesystem-0.24.0 tensorflow-model-optimization-0.7.1 tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109 tf-models-official-2.8.0 tf-slim-1.1.0 tokenizers-0.11.5 torch-1.10.1 torchaudio-0.10.1 torchvision-0.11.2 tqdm-4.62.3 transformers-4.16.2 typeguard-2.13.3 xxhash-2.0.2\n",
      "Removing intermediate container 85c44faf4721\n",
      " ---> 1872b1afaa95\n",
      "Step 5/8 : RUN rm -rf /var/sitecustomize/sitecustomize.py\n",
      " ---> Running in e4d2cf7c6a7d\n",
      "Removing intermediate container e4d2cf7c6a7d\n",
      " ---> 5989293cac8c\n",
      "Step 6/8 : COPY . /trainer\n",
      " ---> 39f7315a9b49\n",
      "Step 7/8 : WORKDIR /trainer\n",
      " ---> Running in 823a8ae01b59\n",
      "Removing intermediate container 823a8ae01b59\n",
      " ---> fa179cb538ce\n",
      "Step 8/8 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in 22e76bd9202a\n",
      "Removing intermediate container 22e76bd9202a\n",
      " ---> 4540dfd26951\n",
      "Successfully built 4540dfd26951\n",
      "Successfully tagged us-central1-docker.pkg.dev/friendly-tower-338419/bertclassifier/bertclassifier:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/friendly-tower-338419/bertclassifier/bertclassifier:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/friendly-tower-338419/bertclassifier/bertclassifier]\n",
      "c068be50dabb: Preparing\n",
      "483cee83e679: Preparing\n",
      "fe7c26a41918: Preparing\n",
      "86f383ca9924: Preparing\n",
      "4c57c3a2fee3: Preparing\n",
      "4c57c3a2fee3: Preparing\n",
      "3dbbc650d5f7: Preparing\n",
      "36c24ce2bec5: Preparing\n",
      "c2847b26212d: Preparing\n",
      "d561bc3142f2: Preparing\n",
      "d561bc3142f2: Preparing\n",
      "53cbd511028e: Preparing\n",
      "5d997b799243: Preparing\n",
      "0d3c6842b77f: Preparing\n",
      "03def71a48c7: Preparing\n",
      "032183a1c75b: Preparing\n",
      "403a518d2b71: Preparing\n",
      "3a88469d4264: Preparing\n",
      "301ef6e29a84: Preparing\n",
      "301ef6e29a84: Preparing\n",
      "c78df0c3ce3c: Preparing\n",
      "27818657c8ea: Preparing\n",
      "e19df3f0bc61: Preparing\n",
      "b70384cdcd53: Preparing\n",
      "7a978fe462d3: Preparing\n",
      "b3cd582c8414: Preparing\n",
      "eb362fcdb550: Preparing\n",
      "f626fcf5cda1: Preparing\n",
      "94d6a6645274: Preparing\n",
      "18803adcd9d2: Preparing\n",
      "a9c44eca1563: Preparing\n",
      "fb04b4bc0668: Preparing\n",
      "e5ccc73dfc63: Preparing\n",
      "bad38e4795f1: Preparing\n",
      "ffbd9d96e302: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "b52f88b681cd: Preparing\n",
      "355ea46b9ff5: Preparing\n",
      "ccc9e6413c69: Preparing\n",
      "21639b09744f: Preparing\n",
      "c2847b26212d: Waiting\n",
      "d561bc3142f2: Waiting\n",
      "53cbd511028e: Waiting\n",
      "5d997b799243: Waiting\n",
      "0d3c6842b77f: Waiting\n",
      "03def71a48c7: Waiting\n",
      "032183a1c75b: Waiting\n",
      "403a518d2b71: Waiting\n",
      "3a88469d4264: Waiting\n",
      "301ef6e29a84: Waiting\n",
      "c78df0c3ce3c: Waiting\n",
      "27818657c8ea: Waiting\n",
      "e19df3f0bc61: Waiting\n",
      "b70384cdcd53: Waiting\n",
      "7a978fe462d3: Waiting\n",
      "b3cd582c8414: Waiting\n",
      "eb362fcdb550: Waiting\n",
      "f626fcf5cda1: Waiting\n",
      "94d6a6645274: Waiting\n",
      "18803adcd9d2: Waiting\n",
      "a9c44eca1563: Waiting\n",
      "fb04b4bc0668: Waiting\n",
      "e5ccc73dfc63: Waiting\n",
      "bad38e4795f1: Waiting\n",
      "ffbd9d96e302: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "b52f88b681cd: Waiting\n",
      "355ea46b9ff5: Waiting\n",
      "ccc9e6413c69: Waiting\n",
      "21639b09744f: Waiting\n",
      "3dbbc650d5f7: Waiting\n",
      "36c24ce2bec5: Waiting\n",
      "c068be50dabb: Pushed\n",
      "86f383ca9924: Pushed\n",
      "483cee83e679: Pushed\n",
      "4c57c3a2fee3: Pushed\n",
      "36c24ce2bec5: Pushed\n",
      "d561bc3142f2: Pushed\n",
      "3dbbc650d5f7: Pushed\n",
      "c2847b26212d: Pushed\n",
      "53cbd511028e: Pushed\n",
      "03def71a48c7: Pushed\n",
      "5d997b799243: Pushed\n",
      "3a88469d4264: Pushed\n",
      "403a518d2b71: Pushed\n",
      "301ef6e29a84: Pushed\n",
      "c78df0c3ce3c: Pushed\n",
      "e19df3f0bc61: Pushed\n",
      "27818657c8ea: Pushed\n",
      "b70384cdcd53: Pushed\n",
      "b3cd582c8414: Pushed\n",
      "eb362fcdb550: Pushed\n",
      "f626fcf5cda1: Pushed\n",
      "94d6a6645274: Pushed\n",
      "18803adcd9d2: Pushed\n",
      "a9c44eca1563: Pushed\n",
      "fb04b4bc0668: Pushed\n",
      "e5ccc73dfc63: Pushed\n",
      "0d3c6842b77f: Pushed\n",
      "ffbd9d96e302: Pushed\n",
      "5f70bf18a086: Layer already exists\n",
      "bad38e4795f1: Pushed\n",
      "7a978fe462d3: Retrying in 5 seconds\n",
      "7a978fe462d3: Retrying in 4 seconds\n",
      "7a978fe462d3: Retrying in 3 seconds\n",
      "7a978fe462d3: Retrying in 2 seconds\n",
      "7a978fe462d3: Retrying in 1 second\n",
      "032183a1c75b: Pushed\n",
      "ccc9e6413c69: Pushed\n",
      "21639b09744f: Pushed\n",
      "b52f88b681cd: Pushed\n",
      "355ea46b9ff5: Pushed\n",
      "fe7c26a41918: Pushed\n",
      "7a978fe462d3: Pushed\n",
      "latest: digest: sha256:f02de56894f3e66f2a819a525fe173487fc40eb2fe7f31c7b9e05b898b0f55d7 size: 8671\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                                                    STATUS\n",
      "497a7b03-64f2-47f3-b4d0-b7e4d8e41c2f  2022-02-17T14:26:36+00:00  28M8S     gs://friendly-tower-338419_cloudbuild/source/1645107995.222383-7ea5a3514cea4fa2ba02954fd0face40.tgz  us-central1-docker.pkg.dev/friendly-tower-338419/bertclassifier/bertclassifier (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR=MODEL_DIR\n",
    "# gcloud builds submit $MODEL_DIR --timeout=200m --config $MODEL_DIR/cloudbuild.yaml\n",
    "!gcloud builds submit {MODEL_DIR} --timeout=150m --config {MODEL_DIR}/cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define a pipeline using the KFP V2 SDK\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model display name: bertclassifier-20220217145448\n",
      "GCS dir for model training artifacts: gs://friendly-tower-338419-vertex-gcs/bertclassifier-20220217145448\n",
      "GCS dir for pipeline artifacts: gs://friendly-tower-338419-vertex-gcs/pipeline_root/mattia_gatto\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# google_cloud_pipeline_components includes pre-built KFP components for interfacing with Vertex AI services.\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import dsl\n",
    "TIMESTAMP=datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "DISPLAY_NAME = MODEL_DIR+\"-{}\".format(TIMESTAMP)\n",
    "GCS_BASE_OUTPUT_DIR= f\"{GCS_BUCKET}/{MODEL_DIR}-{TIMESTAMP}\"\n",
    "\n",
    "USER = \"mattia_gatto\" \n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(GCS_BUCKET, USER)\n",
    "\n",
    "print(f\"Model display name: {DISPLAY_NAME}\")\n",
    "print(f\"GCS dir for model training artifacts: {GCS_BASE_OUTPUT_DIR}\")\n",
    "print(f\"GCS dir for pipeline artifacts: {PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-built Vertex model serving container for deployment.\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=MODEL_DIR, pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    staging_bucket: str = GCS_BUCKET,\n",
    "    display_name: str = DISPLAY_NAME,    \n",
    "    container_uri: str = IMAGE_URI,\n",
    "    model_serving_container_image_uri: str = SERVING_IMAGE_URI,    \n",
    "    base_output_dir: str = GCS_BASE_OUTPUT_DIR,\n",
    "):\n",
    "    \n",
    "    #TODO: add and configure the pre-built KFP CustomContainerTrainingJobRunOp component using\n",
    "    # the remaining arguments in the pipeline constructor. \n",
    "    # Hint: Refer to the component documentation link above if needed as well.\n",
    "    model_train_evaluate_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        # Vertex AI Python SDK authentication parameters.        \n",
    "        project=project,\n",
    "        location=location,\n",
    "        staging_bucket=staging_bucket,\n",
    "        # WorkerPool arguments.\n",
    "        replica_count=1,\n",
    "        machine_type=\"c2-standard-4\",\n",
    "        # TODO: fill in the remaining arguments from the pipeline constructor.\n",
    "        display_name=display_name,\n",
    "        container_uri=container_uri,\n",
    "        model_serving_container_image_uri=model_serving_container_image_uri,\n",
    "        base_output_dir=base_output_dir,\n",
    "    )    \n",
    "    \n",
    "    # Create a Vertex Endpoint resource in parallel with model training.\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        # Vertex AI Python SDK authentication parameters.\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=display_name\n",
    "    \n",
    "    )   \n",
    "    \n",
    "    # Deploy your model to the created Endpoint resource for online predictions.\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        # Link to model training component through output model artifact.\n",
    "        model=model_train_evaluate_op.outputs[\"model\"],\n",
    "        # Link to the created Endpoint.\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        # Define prediction request routing. {\"0\": 100} indicates 100% of traffic \n",
    "        # to the ID of the current model being deployed.\n",
    "        traffic_split={\"0\": 100},\n",
    "        # WorkerPool arguments.        \n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=MODEL_DIR+\".json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Run the pipeline on Vertex Pipelines\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENABLE API:\n",
    "\n",
    "1. https://console.cloud.google.com/apis/enableflow?apiid=ml.googleapis.com,compute_component,containerregistry.googleapis.com&redirect=https:%2F%2Fconsole.cloud.google.com&authuser=1&project=friendly-tower-338419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bertclassifier-20220217165511?project=480829964338\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/480829964338/locations/us-central1/pipelineJobs/bertclassifier-20220217165511 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [customcontainertrainingjob-run].; Job (project_id = friendly-tower-338419, job_id = 8146787425541160960) is failed due to the above error.; Failed to handle the job: {project_number = 480829964338, job_id = 8146787425541160960}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12031/3676697686.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0menable_caching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mvertex_pipelines_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     def submit(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [customcontainertrainingjob-run].; Job (project_id = friendly-tower-338419, job_id = 8146787425541160960) is failed due to the above error.; Failed to handle the job: {project_number = 480829964338, job_id = 8146787425541160960}\"\n"
     ]
    }
   ],
   "source": [
    "#passa a modalità uniforme gcs e abilita acs come permessi\n",
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=MODEL_DIR,\n",
    "    template_path=MODEL_DIR+\".json\",\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"location\": REGION,\n",
    "        \"staging_bucket\": GCS_BUCKET,\n",
    "        \"display_name\": DISPLAY_NAME,        \n",
    "        \"container_uri\": IMAGE_URI,\n",
    "        \"model_serving_container_image_uri\": SERVING_IMAGE_URI,        \n",
    "        \"base_output_dir\": GCS_BASE_OUTPUT_DIR},\n",
    "    enable_caching=True,\n",
    ")\n",
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Query deployed model on Vertex Endpoint for online predictions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your deployed Endpoint name from your pipeline.\n",
    "ENDPOINT_NAME = vertexai.Endpoint.list()[0].name\n",
    "#TODO: Generate online predictions using your Vertex Endpoint.\n",
    "\n",
    "endpoint = vertexai.Endpoint(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "#TODO: write a movie review to test your model e.g. \"The Dark Knight is the best Batman movie!\"\n",
    "test_review = \"The Dark Knight is the best Batman movie!\"\n",
    "\n",
    "prediction =endpoint.predict([test_review])\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
