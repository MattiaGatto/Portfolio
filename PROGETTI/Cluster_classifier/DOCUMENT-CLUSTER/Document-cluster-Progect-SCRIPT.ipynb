{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"HmmXDF4pQZmv"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package names to\n","[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package names is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"name":"stdout","output_type":"stream","text":["Computing: [........................................] 0/7\n","Computing: [#####...................................] 1/7\n","Computing: [##########..............................] 2/7\n","Computing: [################........................] 3/7\n","Computing: [#####################...................] 4/7\n","Computing: [##########################..............] 5/7\n","ricorso epigrafe chiede rsquo ottemperanza giudicato decreto corte d rsquo appello perugia n. 666 5.3.2018 ministero rsquo economia finanze egrave stato condannato pagare favore sig giuseppe nevi titolo indennizzo ex lege 89/2001 somma euro 1.250,00 oltre interessi legali domanda saldo unitamente spese lite pari euro 450,00 oltre iva accessori legge rifondere difensore antistatario avv laura crucianelli anch rsquo ella tal titolo ricorrente chiedono altres igrave ricorrenti caso ulteriore ritardo pagamento somme egrave causa rsquo indennit agrave mora rsquo art 114 comma 4 lett codice processo amministrativo rsquo amministrazione egrave costituita giudizio rilevando cessazione materia contendere ragione rsquo avvenuto pagamento somme argomento camera consiglio giorno 12 ottobre 2001 causa egrave stata trattenuta decisione ograve premesso deve osservarsi ministero intimato provveduto pagamento dovuto ordinativi atti causa resta pertanto dichiarare cessazione materia contendere sensi rsquo art 34 comma 5 codice processo amministrativo ragione mancanza osservazioni senso contrario parte ricorrente va disposta condanna spese lite ministero rsquo economia finanze secondo criterio soccombenza ldquo virtuale rdquo risultando comunque pagamento intervenuto successivamente notifica presente ricorso\n","Computing: [#################################.......] 6/7\n","Computing: [########################################] 7/7\n","                                                    text  \\\n","8343   ricors epigraf chied ottemper giudic decret ap...   \n","11791  ricors epigraf chied ottemper giudic decret ap...   \n","11809  ricors epigraf chied ottemper giudic decret ap...   \n","\n","                                                 cluster        x0        x1  \\\n","8343   Cluster 3 = [rilasc,redd,rinnov,rinnov permess... -0.243214  0.129003   \n","11791  Cluster 3 = [rilasc,redd,rinnov,rinnov permess... -0.246415  0.128073   \n","11809  Cluster 3 = [rilasc,redd,rinnov,rinnov permess... -0.243214  0.129003   \n","\n","                       Topics  \\\n","8343   Risarcimento del danno   \n","11791  Risarcimento del danno   \n","11809  Risarcimento del danno   \n","\n","                                             Insert-text  simil  \\\n","8343   ricorso epigrafe chiede rsquo ottemperanza giu...   1.00   \n","11791  ricorso epigrafe chiede rsquo ottemperanza giu...   0.99   \n","11809  ricorso epigrafe chiede rsquo ottemperanza giu...   1.00   \n","\n","                         Id  \n","8343   ga-tar_pg-2021-825-1  \n","11791  ga-tar_pg-2021-822-1  \n","11809  ga-tar_pg-2021-821-1  \n","\n","\n","Risultato salvato in:  data/risultato.csv\n"]}],"source":["# We install the Simple Transformers library to use Transformer models in a simple way\n","import pandas as pd\n","import os\n","import pickle\n","import string\n","from sklearn.decomposition import PCA\n","from sklearn.metrics.pairwise import cosine_similarity\n","import warnings\n","import nltk\n","import pickle\n","import re\n","nltk.download('names')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import names\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","\n","warnings.simplefilter(\"ignore\")\n","\n","path_model=\"model/\"\n","path_data=\"data/\"\n","\n","compuying_print=[\"Computing: [........................................] 0/7\",\n","\"Computing: [#####...................................] 1/7\",\n","\"Computing: [##########..............................] 2/7\",\n","\"Computing: [################........................] 3/7\",\n","\"Computing: [#####################...................] 4/7\",\n","\"Computing: [##########################..............] 5/7\",\n","\"Computing: [#################################.......] 6/7\",\n","\"Computing: [########################################] 7/7\"]\n","\n","print(compuying_print[0])\n","n_cluster=4\n","\n","f = path_model+\"k-means/\"+\"K_means_\"+ str(n_cluster)\n","if os.path.isfile(f):\n","  with open(f, \"rb\") as f:                                   # riapre il file in lettura ...\n","    kmeans= pickle.load(f)                                   # ... carica il record ...\n","    clusters = kmeans.labels_\n","\n","    print(compuying_print[1])\n","    f.close()\n","\n","if os.path.isfile(path_model+\"tfidf.pickle\"):\n","  with open(path_model+\"tfidf.pickle\", \"rb\") as f:           # riapre il file in lettura ...\n","    tfizer=pickle.load(f)                                    # ... carica il record ...\n","    print(compuying_print[2])\n","    f.close()\n","\n","# inizializziamo la PCA con 2 componenti\n","pca = PCA(n_components=2, random_state=42)\n","if os.path.isfile(path_model+\"pca.pickle\"):\n","  with open(path_model+\"pca.pickle\", \"rb\") as f:             # riapre il file in lettura ...\n","    pca_vecs=pickle.load(f)                                  # ... carica il record ...\n","    print(compuying_print[3])\n","    f.close()\n","\n","n_cluster_key=10\n","if os.path.isfile(path_model+\"cluster_map.pickle\"+str(n_cluster_key)):\n","  with open(path_model+\"cluster_map.pickle\"+str(n_cluster_key), \"rb\") as f:           # riapre il file in lettura ...\n","    cluster_map=pickle.load(f)                                                        # ... carica il record ...\n","    print(compuying_print[4])\n","    f.close()\n","\n","df_sentence = pd.read_csv(path_data+\"outputfile_sentence_processed.csv\")\n","df_sentence = df_sentence.rename(columns={'FactAndLaw': 'text'})\n","if os.path.isfile(path_model+\"X_tfidf.pickle\"):\n","  with open(path_model+\"X_tfidf.pickle\", \"rb\") as f:           # riapre il file in lettura ...\n","    X_tfidf=pickle.load(f)                                     # ... carica il record ...\n","    print(compuying_print[5])\n","    f.close()\n","else:\n","  with open(path_model+\"X_tfidf.pickle\", \"wb\") as f:           # riapre il file in lettura ...\n","    X_tfidf=tfizer.transform(df_sentence['text'])              # ... salva il record ...\n","    pickle.dump(X_tfidf,f)\n","    print(compuying_print[5])\n","    f.close()\n","\n","with open(path_data+\"exception/\"+\"comuni.txt\", \"r\") as tf:\n","    comuni = tf.read().split('\\n')\n","comuni=set(list(x.lower() for x in comuni))\n","with open(path_data+\"exception/\"+\"otherStopwords.txt\", \"r\") as tfo:\n","    otherstopwords = tfo.read().split('\\n')\n","otherstopwords=set(list(x.lower() for x in otherstopwords))\n","with open(path_data+\"exception/\"+\"avverbi_italiani.txt\", \"r\") as tfo:\n","    avverbi_italiani = tfo.read().split('\\n')\n","avverbi_italiani=set(list(x.lower() for x in avverbi_italiani))\n","with open(path_data+\"exception/\"+\"acronimi_.txt\", \"r\") as tfo:\n","    acronimi = tfo.read().split('\\n')\n","acronimi=list(x.lower() for x in acronimi)\n","acronimi=set(list(''.join(filter(lambda x: x not in ['.',' ',',','!','?','#','\\\\','/','^','\\'','\\s','\\n','\\t'], w)) for w in acronimi))\n","with open(path_data+\"exception/\"+\"codifiche_accenti.txt\", \"r\") as tfo:\n","    codifiche_accenti = tfo.read().split('\\n')\n","codifiche_accenti=set(list(x.lower() for x in codifiche_accenti))\n","\n","for _,_,files in os.walk(path_data+\"exception/\"):\n","    for file in files:\n","        if not file in ['acronimi_.txt','avverbi_italiani.txt','codifiche_accenti.txt','comuni.txt','otherStopwords.txt']: \n","            print(f\"Ci troviamo nella cartella: '{file}'\")\n","            with open(path_data+\"exception/\"+file, \"r\") as tfo:\n","                otherstopwords_plus = tfo.read().split('\\n')\n","            otherstopwords=set(list(otherstopwords+otherstopwords_plus))\n","\n","# stopwords list\n","stop_words = set(stopwords.words('italian'))\n","stemmer = SnowballStemmer(\"italian\")\n","nomi_person= set(w.lower() for w in names.words('male.txt')+names.words('female.txt'))\n","\n","eccezioni=set(comuni.union(stop_words.union(codifiche_accenti.union(otherstopwords.union(nomi_person.union(avverbi_italiani.union(acronimi)))))))\n","\n","\n","def preprocessing_single_text(t):\n","    #LOWER_CASE\n","    testo= str(t).lower()\n","\n","    # tokenization\n","    word_tokens = nltk.word_tokenize(testo);\n","\n","    # Rimuovi punteggiatura\n","    word_tokens = list(filter(lambda token: token not in string.punctuation, word_tokens))\n","\n","    # stoppping and stemming\n","    filtered_sentence = [re.sub(r'[0-9]+', '',stemmer.stem(''.join(filter(lambda x: x not in ['.',' ',',','-','!','?','#','\\\\','/','^','\\'','\\s','\\n','\\t'], w)))) \n","                        for w in word_tokens if (not w in eccezioni) \n","                        and (len(stemmer.stem(w))>3) ]\n","    filtered_sentence= [w for w in filtered_sentence if (not w in eccezioni) and (len(w)>3)]\n","\n","    document = ' '.join(filtered_sentence)\n","\n","    return document\n","\n","pca_vecs_D = pca_vecs.transform(X_tfidf.toarray())\n","# salviamo le nostre due dimensioni in x0 e x1\n","x0 = pca_vecs_D[:, 0]\n","x1 = pca_vecs_D[:, 1]\n","df_sentence['cluster'] = kmeans.predict(X_tfidf)\n","df_sentence['cluster'] = df_sentence['cluster'].map(cluster_map)\n","df_sentence['x0'] = x0\n","df_sentence['x1'] = x1\n","\n","# Test value prediction\n","testo=input(\"Inserisci il FactAndLaw da verificare:\")\n","#esempio\n","#testo=\"ricorso epigrafe chiede rsquo ottemperanza giudicato decreto corte d rsquo appello perugia n. 666 5.3.2018 ministero rsquo economia finanze egrave stato condannato pagare favore sig giuseppe nevi titolo indennizzo ex lege 89/2001 somma euro 1.250,00 oltre interessi legali domanda saldo unitamente spese lite pari euro 450,00 oltre iva accessori legge rifondere difensore antistatario avv laura crucianelli anch rsquo ella tal titolo ricorrente chiedono altres igrave ricorrenti caso ulteriore ritardo pagamento somme egrave causa rsquo indennit agrave mora rsquo art 114 comma 4 lett codice processo amministrativo rsquo amministrazione egrave costituita giudizio rilevando cessazione materia contendere ragione rsquo avvenuto pagamento somme argomento camera consiglio giorno 12 ottobre 2001 causa egrave stata trattenuta decisione ograve premesso deve osservarsi ministero intimato provveduto pagamento dovuto ordinativi atti causa resta pertanto dichiarare cessazione materia contendere sensi rsquo art 34 comma 5 codice processo amministrativo ragione mancanza osservazioni senso contrario parte ricorrente va disposta condanna spese lite ministero rsquo economia finanze secondo criterio soccombenza ldquo virtuale rdquo risultando comunque pagamento intervenuto successivamente notifica presente ricorso\"\n","print(testo)\n","\n","X_predict=pd.DataFrame({'text':[preprocessing_single_text(testo)]})\n","X_tfidf_predict = tfizer.transform(X_predict['text'])\n","if os.path.isfile(path_model+\"pca_vecs.pickle\"):\n","  with open(path_model+\"pca_vecs.pickle\", \"rb\") as f:                          # riapre il file in lettura ...\n","    pca_vecs_pred=pickle.load(f)                                               # ... carica il record ...\n","    print(compuying_print[6])\n","    f.close()\n","else:\n","  with open(path_model+\"pca_vecs.pickle\", \"wb\") as f:                          # riapre il file in lettura ...\n","    pca_vecs_pred = pca_vecs.transform(X_tfidf_predict.toarray())              # ... salva il record ...\n","    pickle.dump(pca_vecs_pred,f)\n","    print(compuying_print[6])\n","    f.close()\n","\n","# salviamo le nostre due dimensioni in x0 e x1\n","x0_pred = pca_vecs_pred[:, 0]\n","x1_pred = pca_vecs_pred[:, 1]\n","X_predict['cluster'] = kmeans.predict(X_tfidf_predict)\n","X_predict['cluster'] = X_predict['cluster'].map(cluster_map)\n","X_predict['x0'] = x0_pred\n","X_predict['x1'] = x1_pred\n","\n","\n","df_without_duplicates=pd.read_csv(path_data+\"dataset_topics_GA_10000.csv\",  sep=\";;;;\", encoding=\"utf-8\")\n","df_without_duplicates = df_without_duplicates.drop_duplicates()\n","df_without_duplicates.reset_index(drop=True,inplace=True)\n","\n","def clear_topics(x):\n","    chars = '\"[];\\\\\\r\\\\\\n\\\\'\n","    return x.translate(str.maketrans('', '', chars))\n","df_without_duplicates['Topics']=df_without_duplicates['Topics'].apply(clear_topics)\n","df_without_duplicates=df_without_duplicates.dropna( subset=['FactAndLaw']).reset_index(drop=True)\n","\n","DS=df_sentence\n","DS[\"Topics\"]=df_without_duplicates[\"Topics\"]\n","DS['Insert-text']=testo\n","DS = DS[DS['cluster'] == X_predict['cluster'][0]]\n","X_tfidf_DS = tfizer.transform(DS['text'])\n","\n","\n","simil=[]\n","for text in X_tfidf_DS:\n","  try:\n","    simil.append(cosine_similarity(X_tfidf_predict[0] , text))\n","  except:\n","    print(\"errore di calcolo per 1 riga\")\n","    simil.append([[0]])\n","    \n","print(compuying_print[7])\n","DS[\"simil\"]=simil\n","def val(x):\n","    return round(x[0][0],2)\n","DS[\"simil\"]=DS[\"simil\"].apply(val)\n","DS[\"Id\"]=df_without_duplicates[\"Id\"]\n","print(DS[DS['simil'] >= 0.90])\n","\n","with open(path_data+\"risultato.csv\", \"wb\") as f: \n","  DS[DS['simil'] >= 0.90].to_csv(f)\n","print(\"\\n\\nRisultato salvato in:  \"+path_data+\"risultato.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOjKDkLy7hq8iz7F3sp2nb/","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
